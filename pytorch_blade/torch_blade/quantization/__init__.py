# Copyright 2022 The BladeDISC Authors. All rights reserved.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import torch
from torch_blade.config import Config

try:
    import torch_blade._torch_blade._quantization as _quantization
    _is_available = True

except ImportError as e:
    _is_available = False


def is_available():
    return _is_available

"""
%1 = aten::fake_quantize_per_channel_affine(%x, %4, %4, %4, %4, %4)
      %2 = torch_blade_quantization::placeholder(%1)
      %3 = aten::t(%2)
"""
def _jit_pass_fold_transpose(c_module):
    # pattern_str = """
    # graph(%x : Tensor):
    #   %11 : int = prim::Constant[value=255]()
    #   return (%11)"""
    # pattern = torch.parse_ir(pattern_str)
    # _quantization.fold_transpose(c_module, pattern)
    patterns = [
        ("aten::matmul", 1),
        ("aten::t", 0),
        ("torch_blade_quantization::placeholder", 0),
        ("aten::fake_quantize_per_channel_affine", 0),
        ("prim::Constant", None)
    ]
    graph = c_module.forward.graph
    all_matched = []
    for n in reversed(graph.node_list()):
        cur_idx = 0
        is_match = True
        matched = []
        cur_node = n
        while(cur_idx < len(patterns) or cur_idx is not None):
            cur_pattern_kind = patterns[cur_idx][0]
            next_t_idx = patterns[cur_idx][1]
            if cur_node.kind() == cur_pattern_kind:
                matched.append(cur_node)
                cur_idx = cur_idx + 1
                if next_t_idx is None:
                    break
                else:
                    cur_node = cur_node.input_list()[next_t_idx].node()
            else:
                matched = []
                is_match = False
                break
        if is_match:
            all_matched.append(matched)

    for m in all_matched:
        constant_node = m[-1]
        constant = constant_node.t("value")
        constant_t = torch.t(constant)
        # new_constant_node = graph.create('prim::Constant')
        constant_node.t_("value", constant_t)
        constant_node.output().inferTypeFrom(constant_t)

        aten_t_node = m[1]
        aten_t_node.output().replaceAllUsesWith(aten_t_node.input())
        aten_t_node.destroy()
        s = 1


def _jit_pass_add_placeholder_for_fake_quant(c_module):
    _quantization.add_placeholder_for_fake_quant(c_module)


def _jit_pass_remove_all_placeholder(c_module):
    _quantization.remove_placeholder(c_module)


def _jit_pass_quantization_preprocess(c_module):
    if not _is_available:
        return

    cfg = Config.get_current_context_or_new()
    is_enabled_quantization = cfg.enable_int8
    if is_enabled_quantization:
        # Add placeholder for each fake quant of weight.
        # Or it will be folded by _jit_pass_constant_propagation.
        # TODO: remove this when fake_quant is added to the skip_list
        # of _jit_pass_constant_propagation.
        # https://github.com/pytorch/pytorch/issues/81460
        _jit_pass_add_placeholder_for_fake_quant(c_module)
        _jit_pass_fold_transpose(c_module)
        s = 1


def _jit_pass_quantization_postprocess(c_module):
    if not _is_available:
        return

    cfg = Config.get_current_context_or_new()
    is_enabled_quantization = cfg.enable_int8
    if _is_available and is_enabled_quantization:
        _jit_pass_remove_all_placeholder(c_module)


def is_fake_quant_op(inp_node_kind):
    if not _is_available:
        # If quantization is not available, there should not be
        # fake quant node in the torchscript, because the torchscript
        # is generated by blade_compression. I just copy the name of
        # aten::fake_quantize here to make this util function work for
        # other situation.
        fake_quant_name = [
            "aten::fake_quantize_per_tensor_affine",
            "aten::fake_quantize_per_channel_affine"
        ]
    else:
        fake_quant_name = [
            _quantization.at_fake_quant_per_tensor_affine_name,
            _quantization.at_fake_quant_per_channel_affine_name
        ]

    return inp_node_kind in fake_quant_name
